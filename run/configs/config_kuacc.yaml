setting_name: SCHEMALESS
prep_dir_name: "prep_schemaless" 

data_mode: 'dev' # train, dev or test
dataset: 'bird' # bird or spider
db_ids: ['california_schools']
granularity_level: 'column'
seed: 42

preprocess:
  sub_schema_generator:
    granularity_level: 'column'
    db_ss_configs: "./run/configs/db_ss_configs.yaml"

  sample_generator:
    granularity_level: 'column'
    model: "gemini-2.0-flash" # gpt-4o-mini | gemini-2.0-flash
    column_value_cnt: 5 # number of value for each column provided in the prompt during example generation
    sample_count_edf: 3 # Sample count for each difficulty level (simple, moderate, challenging)
    
  db_completion_dataset:
    db_completion_configs: "./run/configs/db_completion_configs.yaml"

train:
  dataset_tasks: ['t2sws', 't2s'] # ['t2s', 'sl', 'dc', 'slws', 't2sws']
  use_curriculum_learning: False # if false, data points will be shuffled.
  data_portion: [0, 1] # training set data portion
  use_col_value_and_descriptions: False
  use_grpo: False
  use_unsloth: False
  use_reasoning: True
  output_format: "xml" # ['xml', 'text'] # This doesn't matter if use_reasoning false
  shuffle_dataset_tasks_items: True
  base_model_name: "Qwen/Qwen2.5-Coder-3B-Instruct" #"unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit"
  load_in_4bit: False # 4bit quantization to reduce memory usage. Can be False.
  use_lora: False
  train_workers_num: 1
  lora_params:
    lora_r: 64
    lora_alpha: 64
    lora_dropout: 0 # unsloth supports any, but 0 is optimized
    bias: "none"
    lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  
  training_params:
    num_train_epochs: 1 # old_values [1, 3]
    per_device_train_batch_size: 1 # For SFT [2, 4]. For GRPO [1]
    gradient_accumulation_steps: 16 # Effective batch size = batch_size * gradient_accumulation
    per_device_eval_batch_size: 1  # Adjust based on GPU memory # old_values[1, 2, 4]
    eval_accumulation_steps: 8 # If left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but requires more memory).
    learning_rate: 2e-4 # For SFT use 2e-4, 1e-4 or 5e-5 | for GRPO use 5e-6
    lr_scheduler_type: "cosine" # "linear", "cosine", etc. For both SFT and GRPO use cosine
    warmup_ratio: 0.03          # For SFT use 0.03, for GRPO use 0.1
    weight_decay: 0.01          # For SFT use 0.01, for GRPO use 0.1
    optim: "paged_adamw_8bit"   # For SFT use paged_adamw_8bit, for GRPO use adamw_8bit
    logging_steps: 500           # Log metrics every N steps. GRPO use 1
    save_steps: 500             # Save checkpoint every N steps
    save_total_limit: 2         # Keep only the last N checkpoints
    max_seq_length: 8192        # Max sequence length for tokenizer and model. SFT -> 16384 | GRPO -> 32768 or 65536

    

evaluation:
  eval_base_model: True
  use_col_value_and_descriptions: False
  use_schema: True # if eval_base_model=True, then this is automatically made True
  use_grpo: False
  use_reasoning: True  
  output_format: "xml" # ['xml', 'text', 'json']
  use_few_shot: False
  max_new_tokens: 2048
  temperature: [0.0, 0.5, 0.5, 0.8, 0.8, 1.0, 1.0, 1.8, 1.8] # old [0.0, 0.2, 0.5, 0.8, 1.0] 
  top_p: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] # old [1.0, 0.8, 1.0, 1.0, 1.0, 1.0]
  
### MODELS ###
# Qwen/Qwen2.5-Coder-<ParamNo>B-Instruct-<GGUF / AWQ / GPTQ-Int4 / GPTQ-Int8 / >
# Qwen/Qwen2.5-Coder-1.5B-Instruct # 32k Context Lenght
# Qwen/Qwen2.5-Coder-3B-Instruct # 32k Context Length
# Qwen/Qwen2.5-Coder-7B-Instruct # 128k Context Length

### UNSLOTH MODELS ###
## Qwen 2.5 Coder Family

# unsloth/Qwen2.5-Coder-0.5B-Instruct-bnb-4bit
# unsloth/Qwen2.5-Coder-1.5B-Instruct-bnb-4bit
# unsloth/Qwen2.5-Coder-3B-Instruct-bnb-4bit
# unsloth/Qwen2.5-Coder-7B-Instruct-bnb-4bit
# unsloth/Qwen2.5-Coder-14B-Instruct-bnb-4bit

### CODES MODELS
# seeklhy/codes-1b
# seeklhy/codes-3b
# seeklhy/codes-7b

