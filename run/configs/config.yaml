setting_name: SCHEMALESS
prep_dir_name: "prep_schemaless" 

data_mode: 'dev' # train, dev or test
dataset: 'bird' # bird or spider
db_ids: ['california_schools']
granularity_level: 'column'
seed: 42

preprocess:
  db_values_preprocess: 
    signature_size: 100 # Size of the MinHash signature
    n_gram: 3 # N-gram size for the MinHash
    threshold: 0.01 # Threshold for the MinHash lr_scheduler_type

  sub_schema_generator:
    granularity_level: 'column'
    db_ss_configs: "./run/configs/db_ss_configs.yaml"

  sample_generator:
    granularity_level: 'column'
    model: "gemini-2.5-flash" # gpt-4o-mini | gemini-2.0-flash | gemini-2.5-flash | gemini-2.5-pro
    column_value_cnt: 5 # number of value for each column provided in the prompt during example generation
    sample_count_edf: 3 # Sample count for each difficulty level (simple, moderate, challenging)
    eval_in_generation: False
    
  db_completion_dataset:
    db_completion_configs: "./run/configs/db_completion_configs.yaml"

train:
  prompt_temp_name: 'slm_t2s' # ['slm_t2s', 't2s']
  dataset_tasks: ['t2s'] # ['t2s', 'sl', 'dc', 'slws', 't2sws']
  schema_content: 'filtered_schema' # this can be one of ['filtered_schema', 'ground_truth_schema', 'whole_schema']
  use_col_value_and_descriptions: True
  use_few_shot: True
  few_shot_cnt: 6
  use_reasoning_in_few_shots: False
  use_curriculum_learning: False # if false, data points will be shuffled.
  data_portion: [0, 1] # training set data portion
  use_grpo: False
  use_unsloth: True
  use_reasoning: True # training for both reasoning and sql generation
  output_format: "xml" # ['xml', 'text'] # This doesn't matter if use_reasoning false
  shuffle_dataset_tasks_items: True
  base_model_name: "cycloneboy/SLM-SQL-1.5B" # "unsloth/Phi-4-mini-reasoning"  # unsloth/Qwen2.5-Coder-3B-Instruct
  load_in_4bit: False # 4bit quantization to reduce memory usage. Can be False.
  use_lora: True
  train_workers_num: 1
  lora_params:
    lora_r: 32
    lora_alpha: 32
    lora_dropout: 0.05 # unsloth supports any, but 0 is optimized
    bias: "none"
    lora_target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
  
  training_params:
    num_train_epochs: 1 # old_values [1, 3]
    per_device_train_batch_size: 2 # For SFT [2, 4]. For GRPO [1]
    gradient_accumulation_steps: 4 # Effective batch size = batch_size * gradient_accumulation
    per_device_eval_batch_size: 1  # Adjust based on GPU memory # old_values[1, 2, 4]
    eval_accumulation_steps: 8 # If left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but requires more memory).
    learning_rate: 2e-4 # For SFT use 2e-4, 1e-4, 2e-5, 5e-5 | for GRPO use 5e-6
    lr_scheduler_type: "cosine" # "linear", "cosine", etc. For both SFT and GRPO use cosine
    warmup_ratio: 0.1          # For SFT use 0.03, for GRPO use 0.1 ## I have used 0.03 for the SFT until now, but SLM paper uses 0.1 for SFT as well
    weight_decay: 0.01          # For SFT use 0.01, for GRPO use 0.1
    optim: "paged_adamw_8bit"   # For SFT use paged_adamw_8bit, for GRPO use adamw_8bit
    logging_steps: 500           # Log metrics every N steps. GRPO use 1
    save_steps: 500            # Save checkpoint every N steps
    save_total_limit: 2         # Keep only the last N checkpoints
    max_seq_length: 32768        # Max sequence length for tokenizer and model. SFT -> 16384 | GRPO -> 32768 or 65536
    packing: False              # Set to true for SFTTrainer to pack multiple short sequences. This can make trainer 5x faster for short sequences
    num_generations: 8 # For GRPO. Decrease if out of memory
    max_prompt_lenght: 32768 # For GRPO
    max_grad_norm: 0.1 # For GRPO
    reward_functions_names: ["syntax_check_reward",  "exec_acc_reward", "f1_acc_reward", "rlaif_reward", "schema_linking_reward", "n_gram_similarity_reward", "format_reward"] # For GRPO, list of reward functions which are syntax_check_reward,  exec_acc_reward, f1_acc_reward, rlaif_reward, schema_linking_reward, n_gram_similarity_reward, format_reward

vector_db:
  model_provider: "google" 
  model_name_or_path: "models/embedding-001"  
  search_k: 3

prep_few_shots:
  db_ids: ['california_schools']
  embedding_model_provider: "google" 
  embedding_model_name_or_path: "models/embedding-001"  
  search_k: 1
  keyword_extraction_llm_model: "gemini-2.5-flash"
  schema_filterer_llm_model: "gemini-2.5-flash" # "gemini-1.5-pro"

evaluation:
  eval_synth_mode: "test" # ["dev", "test"]
  prompt_temp_name: 'slm_t2s'  # ['csc_t2s', 'slm_t2s', 't2s']
  eval_base_model: True
  use_proprietary_model: False
  proprietary_model_name: "gemini-2.5-flash"
  use_col_value_and_descriptions: False
  use_schema: True # if eval_base_model=True, then this is automatically made True
  schema_content: 'filtered_schema' # this can be one of ['filtered_schema', 'ground_truth_schema', 'whole_schema']
  use_few_shot: False
  few_shot_cnt: 0
  use_reasoning_in_few_shots: False
  use_reasoning: True  
  output_format: "xml" # ['xml', 'text', 'json']
  max_new_tokens: 4096
  use_grpo: False
  use_unsloth: False
  temperature: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] # [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] # [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]  # old1: [0.0, 0.5, 0.5, 0.8, 0.8, 1.0, 1.0, 1.8, 1.8] ||||| [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]
  top_p: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  # [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] # old [1.0, 0.8, 1.0, 1.0, 1.0, 1.0]
  keyword_extraction_llm_model: "gemini-2.5-flash" # Either empty or proprietary models. If you set empty, it will use the model above
  eval_no: 4
  
### SLM MODELS
# cycloneboy/SLM-SQL-Base-0.5B ## qwen coder based
# cycloneboy/SLM-SQL-0.5B ## qwen coder based
# cycloneboy/SLM-SQL-1.5B ## qwen coder based
# cycloneboy/SLM-SQL-Base-1.5B ## qwen coder based
# cycloneboy/SLM-SQL-Base-1.3B ## deepseek coder based
# cycloneboy/SLM-SQL-1.3B ## deepseek coder based

### OmniSQL models
# seeklhy/OmniSQL-7B
# seeklhy/OmniSQL-14B
# seeklhy/OmniSQL-32B

### CSC-Models
# cycloneboy/CscSQL-Grpo-Qwen2.5-Coder-3B-Instruct
# cycloneboy/CscSQL-Grpo-XiYanSQL-QwenCoder-3B-2502
# cycloneboy/CscSQL-Grpo-Qwen2.5-Coder-7B-Instruct
# cycloneboy/CscSQL-Grpo-XiYanSQL-QwenCoder-7B-2502